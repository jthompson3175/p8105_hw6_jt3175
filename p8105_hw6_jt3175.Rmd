---
title: "Homework 6"
author: "Julia Thompson"
date: "11/20/2019"
output:
  github_document:
   pandoc_args: --webtex
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1

We load and clean the data for regression analysis, and find that there are no missing values in any of the columns. We also recode and convert _babysex_, _frace_, _malform_, and _mrace_ to factor variables. 

```{r}
birthweight = read_csv("./data/birthweight.csv") 

ismissing = map(.x = birthweight, ~ sum(is.na(.x))) %>%  
  bind_cols()

# Recode variables to make sense

birthweight = birthweight %>% 
  mutate(
    babysex = recode(babysex, `1` = "male", `2` = "female"),
    frace = recode(frace, `1` = "white", `2` = "black", `3` = "asian", `4` = "puerto rican", `8` = "other", `9` = "unknown"),
    malform = recode(malform, `0` = "absent", `1` = "present"),
    mrace = recode(mrace,  `1` = "white", `2` = "black", `3` = "asian", `4` = "puerto rican", `8` = "other")
  )

# change the above recoded variables into factors with levels corresponding to the above order

birthweight = birthweight %>% 
  mutate(
    babysex = factor(babysex, levels = c("male", "female")),
    frace = factor(frace, levels = c("white", "black", "asian", "puerto rican", "other", "unknown")),
    malform = factor(malform, levels = c("absent", "present")),
    mrace = factor(mrace, levels = c("white", "black", "asian", "puerto rican", "other"))
  )
```

We used a hypothesized structure consulting literature for the factors that underly birthweight to reduce our number of potential predictors. We will continue with the following: gender (_babysex_), mother's BMI (_ppbmi_), mother's height (_mheigth_), cigarettes smoked per day (_smoken_), and mother's age (_momage_). We chose a final model by fitting an interaction term between mother's age and number of cigarettes smoked per day, as indicated by past research. Because the interaction term is significant and there is evidence in the literature of interaction between age and smoking, we will keep the interaction in our final model. We then create a plot of residuals vs fitted values and find that our points are centered around 0 with no clear patterns that would be problematic. 

```{r}
# interaction between smoking status and age - this interaction term is significant, so we will keep this as our final model

interaction_model = lm(bwt ~ babysex + ppbmi + mheight + momage*smoken, data = birthweight) 
summary(interaction_model)

# show a plot of residuals vs fitted

birthweight = 
  birthweight %>% 
  add_residuals(interaction_model) %>% 
  add_predictions(interaction_model)

# residuals are clustered, but there is no clear pattern... seems ok enough to proceed

ggplot(birthweight, aes(x = pred, y = resid))+
  geom_point(alpha = .4)+
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  )
```

We compare the above model to two others using cross validation:

* One using length at birth and gestational age as predictors
* One using head circumference, length, sex, and all interactions between these

To do this we create 100 testing and training datasets, where each one is split 80% training and 20% testing. We then fit each model using the training dataset, and evaluate it using the prediction error from the testing dataset. We do this for each of the 100 datasets, and then create violin plots comparing the cross-validated prediciton error. 

We find that the more complex model has the lowest distribution of prediction errors. 

```{r}
# need to create testing and training datasets to use on all 3 models.

cv_df = crossv_mc(birthweight, 100)

cv_df = cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

```

```{r}
# Now fit the respective models to each

cv_df = 
  cv_df %>% 
  mutate(interaction_model = map(train, ~lm(bwt ~ babysex + ppbmi + mheight + momage * smoken, data = .x)),
         simple_model_given = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         complex_model_given = map(train, ~lm(bwt ~ babysex * bhead * blength, data = .x))) %>% 
  mutate(rmse_interaction = map2_dbl(interaction_model, test, ~rmse(model = .x, data = .y)),
         rmse_simple = map2_dbl(simple_model_given, test, ~rmse(model = .x, data = .y)),
         rmse_complex = map2_dbl(complex_model_given, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  labs(title = "Cross-Validated Prediction Error for Three Models",
       x = "Model",
       y = "RMSE")
```

## Problem 2

```{r, include = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

We focus on a simple linear regression with tmax as the response and tmin as the predictor. We will use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of $\hat{r}^2\ \text{and}\ log(\hat{{\beta}_0} * \hat{{\beta}_1})$.

```{r}
bootstrap = 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x))
  )

output = 
  bootstrap %>% 
  mutate(
    results_ln = map(models, broom::tidy),
    results_rsq = map(models, broom::glance)
    ) %>% 
  select(-strap, -models) %>%
  unnest(results_ln, results_rsq) %>% 
  select(.id, term, estimate, adj.r.squared) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    ln_est = log(intercept * tmin)
  ) %>% 
  select(adj_r_squared, ln_est)
```

We plot the distribution of $\hat{r}^2\ \text{and}\ log(\hat{{\beta}_0} * \hat{{\beta}_1})$, shown below. When using a large number of bootstrap samples, such as 5000, the distrubutions of our estimates are approximately normal. We see that $\hat{r}^2$ is centered at about 0.91, and $log(\hat{{\beta}_0} * \hat{{\beta}_1})$ is centered at about 2.02. 

```{r}
ggplot(output, aes(x = adj_r_squared)) +
  geom_histogram() +
  labs(title = "Distribution of Adjusted R Squared",
       x = "Adjusted R Squared",
       y = "Count")

ggplot(output, aes(x = adj_r_squared)) +
  geom_density() +
  labs(title = "Distribution of Adjusted R Squared",
       x = "Adjusted R Squared",
       y = "Density")

ggplot(output, aes(x = ln_est)) +
  geom_histogram() +
  labs(title = "Distribution of Log of Beta 1 * Beta 0",
       x = "Log of Beta 1 * Beta 0",
       y = "Count")

ggplot(output, aes(x = ln_est)) +
  geom_density() +
  labs(title = "Distribution of Log of Beta 1 * Beta 0",
       x = "Log of Beta 1 * Beta 0",
       y = "Density")
```

Using the 5000 bootstrap estimates, we identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval on $\hat{r}^2\ \text{and}\ log(\hat{{\beta}_0} * \hat{{\beta}_1})$. The below table gives the 95% confidence intervals for each respective estimate. 

```{r}
conf_int = output %>% 
  summarize(
    lower_rsq = quantile(adj_r_squared, .025),
    upper_rsq = quantile(adj_r_squared, .975),
    lower_ln = quantile(ln_est, .025),
    upper_ln = quantile(ln_est, .975)
  ) %>% 
  pivot_longer(
    cols = lower_rsq:upper_ln,
    names_to = "Estimate",
    values_to = c("Lower", "Upper"),
    names_prefix = c("lower_", "upper_")
  ) %>% 
  mutate(
    Estimate = recode(Estimate, "rsq" = "R Hat Squared", "ln" = "Log Beta Hat")
  )

conf_int %>% knitr::kable()
```

